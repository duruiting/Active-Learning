{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy随机数置为1\n",
    "np.random.seed(1)\n",
    "#为CPU设置种子用于生成随机数，以使得结果是确定的\n",
    "torch.manual_seed(1)\n",
    "#GPU随机数种子置为1\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据并转为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index=pkl.load(open(\"./data/vocab.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size=32\n",
    "#读取数据并转成索引\n",
    "def read_text(path):\n",
    "    contents,labels=[],[]\n",
    "    with open(path,\"r\",encoding=\"UTF-8\") as file:\n",
    "        for line in file:\n",
    "            #print(line.split(\"\\t\"))\n",
    "            #获取数据以及数据的标签\n",
    "            sentence,label=line.split(\"\\t\")[0],line.split(\"\\t\")[1]\n",
    "            #将文本数据按字分词\n",
    "            sentence_list=[]\n",
    "            for i in sentence:\n",
    "                sentence_list.append(i)\n",
    "            #pad_size=32，不够的补齐，超出的去掉\n",
    "            if len(sentence_list)<pad_size:\n",
    "                sentence_list.extend([word_to_index[\"<PAD>\"]] * (pad_size - len(sentence_list)))\n",
    "            else:\n",
    "                sentence_list=sentence_list[:pad_size]\n",
    "                #print(len(sentence_list),sentence_list)\n",
    "            #都转成固定的pad_size大小后，然后将其转为语料库的index\n",
    "            res_list=[]\n",
    "            for word in sentence_list:\n",
    "                if word == 4761:\n",
    "                    res_list.append(4761)\n",
    "                    continue\n",
    "                try: # 能在语料库中找到这个字的添加其索引\n",
    "                    res_list.append(word_to_index[word])\n",
    "                except: # 语料库里没有的字换成<UNK>的索引\n",
    "                    res_list.append(word_to_index[\"<UNK>\"])\n",
    "            #print(res_list,label)\n",
    "            contents.append(res_list)\n",
    "            labels.append(int(label))\n",
    "    return contents,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是主动学习，这里我们采用验证集的数据作为模型训练数据，训练集的数据作为模型的未标注数据，测试集数据还作为测试集数据进行模型效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data,unlabeled_label=read_text(\"./data/train.txt\")\n",
    "train_data,train_label=read_text(\"./data/dev.txt\")\n",
    "test_data,test_label=read_text(\"./data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 180000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(unlabeled_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [173, 714, 3, 186, 1844, 889, 0, 2641, 80, 2061, 416, 478, 382, 5, 308, 15, 1264, 1344, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761]\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data),train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pretrained = torch.tensor(np.load( './data/embedding_SougouNews.npz')[\"embeddings\"].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4762, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_pretrained.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class=len([x.strip() for x in open('./data/class.txt').readlines()])\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding.from_pretrained(embedding_pretrained, freeze=False)\n",
    "        self.lstm=nn.LSTM(input_size=embedding_pretrained.size(1),hidden_size=128 ,num_layers=2,bidirectional=True, batch_first=True, dropout=0.2)\n",
    "        self.fc=nn.Linear(128*2,num_class)\n",
    "        #损失预测层\n",
    "        self.loss_fc=nn.Linear(128*2*2,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.embedding(x)  # out:[batch_size, seq_len, embedding]=[128, 32, 300],这也是LSTM层(batch_first=True)的输入格式\n",
    "        out, _ = self.lstm(out)#out接收output，_接收元组(h_n,c_n)\n",
    "        #定义损失预测层的输入\n",
    "        loss_input=torch.cat((out[:, 0, :],out[:, -1, :]),1)\n",
    "        #print(out[:, 0, :].shape,out[:, -1, :].shape,loss_input.shape)\n",
    "        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state\n",
    "        #损失预测层最终的loss预测值\n",
    "        loss_out=self.loss_fc(loss_input)\n",
    "        return out,loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm=LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss、optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(lstm.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm层的loss\n",
    "lstm_loss_fn=nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失预测层的loss,公式里衡量参数 ξ=1\n",
    "def loss_loss_fn(outputs,labels,loss_outputs):\n",
    "    res=torch.tensor(0,dtype=torch.float32)\n",
    "    res=res.to(device)\n",
    "    #res=0\n",
    "    for i in range(0,len(outputs),2):\n",
    "        true_loss1=lstm_loss_fn(outputs[i].unsqueeze(0), labels[i].unsqueeze(0))\n",
    "        true_loss2=lstm_loss_fn(outputs[i+1].unsqueeze(0), labels[i+1].unsqueeze(0))\n",
    "        temp=0\n",
    "        if true_loss1>true_loss2:\n",
    "            temp=1\n",
    "        else:\n",
    "            temp=-1\n",
    "        pred_loss=max(torch.tensor(0,dtype=torch.float32),-temp*(loss_outputs[i][0]-loss_outputs[i+1][0])+1)\n",
    "        res+=pred_loss\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data,label,batch_size=64,shuffle=True):\n",
    "    tensor_data, tensor_label = map(torch.tensor, (data, label))\n",
    "    dataset = TensorDataset(tensor_data, tensor_label)\n",
    "    dataloader=DataLoader(dataset,num_workers=4, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data,label,num_epochs):\n",
    "    model.train()\n",
    "    batch_size=64\n",
    "    train_iter=batch_data(data,label,batch_size)\n",
    "    \n",
    "    for index,(trains,labels) in enumerate(train_iter):\n",
    "        trains=trains.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs,loss_outputs = model(trains)\n",
    "        #print(outputs.shape[0])\n",
    "        if outputs.shape[0]%2 != 0:\n",
    "            print(\"batch_size为奇数，不执行\")\n",
    "        else:\n",
    "            #print(outputs.shape,loss_outputs.shape)#torch.Size([batch_size, 10]) torch.Size([batch_size, 1])\n",
    "            #获得一个batch数据真实的loss\n",
    "            true_loss=lstm_loss_fn(outputs, labels)\n",
    "            #获得一个batch数据预测的loss\n",
    "            pre_loss_sum=loss_loss_fn(outputs,labels,loss_outputs)\n",
    "            #print(true_loss,pre_loss_sum)\n",
    "            loss=true_loss+0.1*2*pre_loss_sum/len(trains)\n",
    "            #loss=loss.to(device) 若代码执行出错报gpu的错误，将这句加上\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data,label):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    test_iter=batch_data(data,label)\n",
    "    accuracy=0\n",
    "    with torch.no_grad():\n",
    "        for trains,labels in test_iter:\n",
    "            trains=trains.to(device)\n",
    "            labels=labels.to(device)\n",
    "            outputs,loss_outputs = model(trains)\n",
    "            loss = lstm_loss_fn(outputs, labels)\n",
    "            test_loss+=loss.item()\n",
    "            \n",
    "            #计算测试集的准确率,计算准确率时不能在gpu上计算，得转到cpu上\n",
    "            true = labels.data.cpu()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "            train_acc = metrics.accuracy_score(true, predic)\n",
    "            #print(train_acc)\n",
    "            accuracy+=train_acc\n",
    "        print(\"平均accuracy:\",accuracy/len(test_iter),\"平均loss:\",test_loss/len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(model,train_data,train_label,unlabeled_data,unlabeled_label):\n",
    "    model.eval()\n",
    "    batch_size=64\n",
    "    with torch.no_grad():\n",
    "        unlabeled_iter=batch_data(unlabeled_data,unlabeled_label,batch_size,False)\n",
    "        res_dict={}\n",
    "        res_list=[]\n",
    "        index=0\n",
    "        for trains,labels in unlabeled_iter:\n",
    "            trains=trains.to(device)\n",
    "            labels=labels.to(device)\n",
    "            outputs,loss_outputs = model(trains)\n",
    "            #下标当作k，loss值当作v\n",
    "            for i in range(len(loss_outputs)):\n",
    "                res_dict[index+i]=loss_outputs[i].item()\n",
    "            index=index+batch_size\n",
    "        # 按v值降序降序,排序后变成元组(k,v)组成的list：[('key1', 3), ('key2', 2), ('key3', 1)]\n",
    "        res_dict=sorted(res_dict.items(),key=lambda x:x[1],reverse = True)\n",
    "        #每次主动学习取出1000个值加入到标记数据集中，并从未标记数据集中删除\n",
    "        num=0\n",
    "        for con in res_dict:\n",
    "            if num >= 1000:\n",
    "                break\n",
    "            res_list.append(con[0])\n",
    "            num+=1\n",
    "        #往标注数据集中添加元素\n",
    "        for i in res_list:\n",
    "            train_data.append(unlabeled_data[i])\n",
    "            train_label.append(unlabeled_label[i])\n",
    "        #从无标注数据集中删除元素(先删角标大的)\n",
    "        for i in sorted(res_list, reverse=True):\n",
    "            del(unlabeled_data[i])\n",
    "            del(unlabeled_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model,train_data,train_label,test_data,test_label,unlabeled_data,unlabeled_label,num_epochs):\n",
    "    model.to(device)\n",
    "    start_time=time.time()\n",
    "    \n",
    "    #run\n",
    "    for epoch in range(num_epochs):\n",
    "        #训练\n",
    "        print(\"第{}次train_data和train_label数量:\".format(epoch),len(train_data),len(train_label))\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, num_epochs),\"用时：\",get_time_dif(start_time))\n",
    "        train(model,train_data,train_label,num_epochs)\n",
    "        #使用测试数据测试模型效果\n",
    "        test(model,test_data,test_label)\n",
    "        #active learning\n",
    "        active_learning(model,train_data,train_label,unlabeled_data,unlabeled_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次train_data和train_label数量: 10000 10000\n",
      "Epoch [1/10] 用时： 0:00:00\n",
      "平均accuracy: 0.31468949044585987 平均loss: 1.6836314892313282\n",
      "第1次train_data和train_label数量: 11000 11000\n",
      "Epoch [2/10] 用时： 0:01:00\n",
      "平均accuracy: 0.5592157643312102 平均loss: 1.2530128948248116\n",
      "第2次train_data和train_label数量: 12000 12000\n",
      "Epoch [3/10] 用时： 0:02:01\n",
      "平均accuracy: 0.7203423566878981 平均loss: 0.8462304488109176\n",
      "第3次train_data和train_label数量: 13000 13000\n",
      "Epoch [4/10] 用时： 0:03:03\n",
      "平均accuracy: 0.7852308917197452 平均loss: 0.6804901570271534\n",
      "第4次train_data和train_label数量: 14000 14000\n",
      "Epoch [5/10] 用时： 0:04:07\n",
      "平均accuracy: 0.8212579617834395 平均loss: 0.578421653740725\n",
      "第5次train_data和train_label数量: 15000 15000\n",
      "Epoch [6/10] 用时： 0:05:14\n",
      "平均accuracy: 0.8264331210191083 平均loss: 0.565793888014593\n",
      "第6次train_data和train_label数量: 16000 16000\n",
      "Epoch [7/10] 用时： 0:06:22\n",
      "平均accuracy: 0.8359872611464968 平均loss: 0.5414501155257985\n",
      "第7次train_data和train_label数量: 17000 17000\n",
      "Epoch [8/10] 用时： 0:07:29\n",
      "平均accuracy: 0.8462380573248408 平均loss: 0.5107007655937961\n",
      "第8次train_data和train_label数量: 18000 18000\n",
      "Epoch [9/10] 用时： 0:08:41\n",
      "平均accuracy: 0.8572850318471338 平均loss: 0.5036855809817649\n",
      "第9次train_data和train_label数量: 19000 19000\n",
      "Epoch [10/10] 用时： 0:09:54\n",
      "平均accuracy: 0.8598726114649682 平均loss: 0.4984431587586737\n"
     ]
    }
   ],
   "source": [
    "run(lstm,train_data,train_label,test_data,test_label,unlabeled_data,unlabeled_label,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
